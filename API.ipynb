{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194ed83a-62a9-4584-9612-efcda5919d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping article 1: [Removed] (https://removed.com)\n",
      "\n",
      "        Aid: 1\n",
      "        Title: removed.com\n",
      "        Date: Unknown\n",
      "        Publisher: Unknown\n",
      "        Views: 0\n",
      "        Comments Count: 0\n",
      "        Content:\n",
      "        \n",
      "        Comments Section:\n",
      "        \n",
      "        \n",
      "\n",
      "------------------------\n",
      "\n",
      "Scraping article 2: Lasers Are Making It Easier to Find Buried Land Mines (https://www.wired.com/story/this-laser-system-can-locate-landmines-with-high-accuracy/)\n",
      "\n",
      "        Aid: 2\n",
      "        Title: Lasers Are Making It Easier to Find Buried Land Mines | WIRED\n",
      "        Date: Unknown\n",
      "        Publisher: Unknown\n",
      "        Views: 0\n",
      "        Comments Count: 0\n",
      "        Content:\n",
      "        \n",
      "        Comments Section:\n",
      "        \n",
      "        \n",
      "\n",
      "------------------------\n",
      "\n",
      "Scraping article 3: [Removed] (https://removed.com)\n",
      "\n",
      "        Aid: 3\n",
      "        Title: removed.com\n",
      "        Date: Unknown\n",
      "        Publisher: Unknown\n",
      "        Views: 0\n",
      "        Comments Count: 0\n",
      "        Content:\n",
      "        \n",
      "        Comments Section:\n",
      "        \n",
      "        \n",
      "\n",
      "------------------------\n",
      "\n",
      "Scraping article 4: FCC passes auto safety spectrum rules (https://www.theverge.com/2024/11/21/24302733/fcc-cv2x-cellular-vehicle-everything-spectrum-rules-final)\n",
      "\n",
      "        Aid: 4\n",
      "        Title: FCC passes auto safety spectrum rules - The Verge\n",
      "        Date: Unknown\n",
      "        Publisher: Unknown\n",
      "        Views: 0\n",
      "        Comments Count: 0\n",
      "        Content:\n",
      "        \n",
      "        Comments Section:\n",
      "        \n",
      "        \n",
      "\n",
      "------------------------\n",
      "\n",
      "Scraping article 5: The Beginning of the End of Big Tech (https://www.wired.com/story/the-beginning-of-the-end-of-big-tech/)\n",
      "\n",
      "        Aid: 5\n",
      "        Title: The Beginning of the End of Big Tech | WIRED\n",
      "        Date: Unknown\n",
      "        Publisher: Unknown\n",
      "        Views: 0\n",
      "        Comments Count: 0\n",
      "        Content:\n",
      "        \n",
      "        Comments Section:\n",
      "        \n",
      "        \n",
      "\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "class Comment:\n",
    "    def __init__(self, comment_id: int, user: str, comment_text: str):\n",
    "        self.comment_id = comment_id\n",
    "        self.user = user\n",
    "        self.comment_text = comment_text\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "        Comment ID: {self.comment_id}\n",
    "        User: {self.user}\n",
    "        Comment:\n",
    "        {self.comment_text}\n",
    "        \"\"\"\n",
    "\n",
    "class ScrapedData:\n",
    "    def __init__(self, \n",
    "                 aid: int, \n",
    "                 title: str, \n",
    "                 date: str, \n",
    "                 publisher: str, \n",
    "                 views: int, \n",
    "                 comments_count: int, \n",
    "                 content: str, \n",
    "                 comments: List[Comment]) -> None:\n",
    "        self.aid = aid\n",
    "        self.title = title\n",
    "        self.date = date\n",
    "        self.publisher = publisher\n",
    "        self.views = views\n",
    "        self.comments_count = comments_count\n",
    "        self.content = content\n",
    "        self.comments = comments\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        comments_str = \"\\n\".join(str(comment) for comment in self.comments)\n",
    "        return f\"\"\"\n",
    "        Aid: {self.aid}\n",
    "        Title: {self.title}\n",
    "        Date: {self.date}\n",
    "        Publisher: {self.publisher}\n",
    "        Views: {self.views}\n",
    "        Comments Count: {self.comments_count}\n",
    "        Content:\n",
    "        {self.content}\n",
    "        Comments Section:\n",
    "        {comments_str}\n",
    "        \"\"\"\n",
    "\n",
    "def scrape_comments(soup: BeautifulSoup) -> List[Comment]:\n",
    "    comments: List[Comment] = []\n",
    "    comments_container = soup.find('div', id='comment_ul')\n",
    "    if not comments_container:\n",
    "        return comments\n",
    "\n",
    "    comment_tags = comments_container.find_all(['dl', 'dI'], id=True)\n",
    "    for comment_tag in comment_tags:\n",
    "        comment_id_str = comment_tag.get('id', '').replace('comment_', '').split('_')[0]\n",
    "        comment_id = int(comment_id_str) if comment_id_str.isdigit() else 0\n",
    "\n",
    "        user_tag = comment_tag.find('a', class_='xi2')\n",
    "        user = user_tag.text.strip() if user_tag else \"Anonymous\"\n",
    "\n",
    "        comment_text_tag = comment_tag.find('dd')\n",
    "        if comment_text_tag:\n",
    "            quote_tags = comment_text_tag.find_all('div', class_='quote')\n",
    "            for quote_tag in quote_tags:\n",
    "                quote_tag.extract()\n",
    "            comment_text = comment_text_tag.get_text(strip=True)\n",
    "        else:\n",
    "            comment_text = \"No comment text\"\n",
    "\n",
    "        comments.append(Comment(comment_id=comment_id, user=user, comment_text=comment_text))\n",
    "    return comments\n",
    "\n",
    "def scrape_article(url: str, aid: int) -> ScrapedData:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = str(soup.find('title').text if soup.find('title') else \"Unknown\")\n",
    "\n",
    "    date_tag = soup.find('p', class_='xg1')\n",
    "    date = str(date_tag.text.split('|')[0].strip()) if date_tag else \"Unknown\"\n",
    "\n",
    "    publisher_tag = date_tag.find('a') if date_tag else None\n",
    "    publisher = str(publisher_tag.text if publisher_tag else \"Unknown\")\n",
    "\n",
    "    views_tag = soup.find('em', id='_viewnum')\n",
    "    views_str = views_tag.text if views_tag else \"0\"\n",
    "    views = int(views_str.replace(',', '')) if views_str.isdigit() else 0\n",
    "\n",
    "    comments_tag = soup.find('em', id='_commentnum')\n",
    "    comments_count_str = comments_tag.text if comments_tag else \"0\"\n",
    "    comments_count = int(comments_count_str.replace(',', '')) if comments_count_str.isdigit() else 0\n",
    "\n",
    "    content_tag = soup.find('td', id='article_content')\n",
    "    content = str(content_tag.get_text(strip=False)) if content_tag else \"\"\n",
    "\n",
    "    comments = scrape_comments(soup)\n",
    "\n",
    "    return ScrapedData(aid, title, date, publisher, views, comments_count, content, comments)\n",
    "\n",
    "def fetch_articles_from_newsapi(api_key: str, query: str, page_size: int = 5):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&pageSize={page_size}&apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'articles' not in data:\n",
    "        raise ValueError(\"Error fetching articles from NewsAPI: \" + data.get('message', 'Unknown error'))\n",
    "\n",
    "    return [(article['url'], article['title']) for article in data['articles']]\n",
    "\n",
    "def main():\n",
    "    api_key = \"3e0754c21dbb4aae92e463269d0830da\"\n",
    "    query = \"technology\"  # Example query to fetch tech-related articles\n",
    "\n",
    "    articles = fetch_articles_from_newsapi(api_key, query)\n",
    "\n",
    "    for i, (url, title) in enumerate(articles, start=1):\n",
    "        print(f\"Scraping article {i}: {title} ({url})\")\n",
    "        try:\n",
    "            scraped_data = scrape_article(url, i)\n",
    "            print(scraped_data)\n",
    "            print(\"\\n------------------------\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ebe56-9273-41b2-a081-362ca38fb9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
