{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82376034-5bd5-4d8a-acee-919ed24e3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "from pyspark.sql.functions import udf, split, col, concat, regexp_replace, explode\n",
    "from typing import List, Optional\n",
    "from ForumClasses import Scraped_Data, Comment\n",
    "from ForumScraper import scrape_article\n",
    "from UtilsRedis import save_word_frequencies_to_redis\n",
    "import redis\n",
    "\n",
    "# PySpark setup\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ScrapedDataProcessor\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2086dd-d0cf-4eb1-86d2-9dc6d0674454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL and AID range\n",
    "base_url = \"https://b.cari.com.my/portal.php?mod=view&aid=\"\n",
    "aid_values = list(range(1, 50))  # Adjust range as needed\n",
    "\n",
    "# UDF to scrape article and comments\n",
    "def scrape_data_udf(aid: int) -> Optional[tuple]:\n",
    "    url = f\"{base_url}{aid}\"\n",
    "    try:\n",
    "        print(f\"Scraping AID: {aid}\")\n",
    "        scraped_data = scrape_article(url, aid)\n",
    "        if scraped_data:\n",
    "            article = scraped_data[:-1]  # Exclude comments\n",
    "            comments = scraped_data[-1]  # Extract comments\n",
    "            return article, comments\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping AID {aid}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Registering UDF\n",
    "scrape_data = udf(scrape_data_udf, StructType([\n",
    "    StructField(\"Article\", StructType([\n",
    "        StructField(\"AID\", IntegerType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Date\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Views\", IntegerType(), True),\n",
    "        StructField(\"Comments_Count\", IntegerType(), True),\n",
    "        StructField(\"Content\", StringType(), True),\n",
    "    ]), True),\n",
    "    StructField(\"Comments\", ArrayType(StructType([\n",
    "        StructField(\"AID\", IntegerType(), True),\n",
    "        StructField(\"Comment_ID\", IntegerType(), True),\n",
    "        StructField(\"User\", StringType(), True),\n",
    "        StructField(\"Comment_Text\", StringType(), True),\n",
    "    ])), True)\n",
    "]))\n",
    "\n",
    "# Creating an AID DataFrame for parallel processing\n",
    "aid_df = spark.createDataFrame([(aid,) for aid in aid_values], [\"AID\"])\n",
    "\n",
    "# Applying the UDF to scrape data\n",
    "scraped_df = aid_df.withColumn(\"ScrapedData\", scrape_data(\"AID\"))\n",
    "\n",
    "# Extracting articles and comments\n",
    "article_df = scraped_df.selectExpr(\"ScrapedData.Article AS Article\").select(\n",
    "    \"Article.*\"\n",
    ")\n",
    "comments_df = scraped_df.selectExpr(\"ScrapedData.Comments AS Comments\").selectExpr(\n",
    "    \"explode(Comments) AS Comment\"\n",
    ").select(\n",
    "    \"Comment.*\"\n",
    ")\n",
    "\n",
    "# Displaying data\n",
    "print(\"Articles DataFrame:\")\n",
    "article_df.show(50, truncate=True)\n",
    "\n",
    "print(\"Comments DataFrame:\")\n",
    "comments_df.show(50, truncate=True)\n",
    "\n",
    "# Writing DataFrames to CSV files with proper handling of quoted fields\n",
    "article_df.write.option(\"header\", True) \\\n",
    "    .option(\"quoteAll\", True) \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"assignData/articles_data_csv\")\n",
    "\n",
    "comments_df.write.option(\"header\", True) \\\n",
    "    .option(\"quoteAll\", True) \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"assignData/comments_data_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab19d3-98b6-4125-9a68-475c0ec1350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_csv = spark.read.csv('assignData/articles_data_csv', header=True)\n",
    "comments_csv = spark.read.csv('assignData/comments_data_csv', header=True)\n",
    "\n",
    "print(\"article_csv.count(): \", article_csv.count())\n",
    "print(\"comments_csv.count(): \", comments_csv.count())\n",
    "\n",
    "article_csv.count()\n",
    "filtered_article_csv = article_csv.filter((col(\"views\").cast(\"int\").isNotNull()) & (col(\"views\") > 0))\n",
    "filtered_article_csv.count()\n",
    "\n",
    "# Concatenate Title and Content columns and replace commas with spaces\n",
    "article_csv_words = filtered_article_csv.withColumn(\"Combined_Text\", concat(col(\"Title\"), col(\"Content\"))) \\\n",
    "                                       .withColumn(\"Combined_Text\", regexp_replace(col(\"Combined_Text\"), \", \", \" \")) \\\n",
    "                                       .withColumn(\"Combined_Words\", split(col(\"Combined_Text\"), \" \"))\n",
    "\n",
    "# Split Comment Text into words by replacing commas with spaces\n",
    "comments_csv_words = comments_csv.withColumn(\"Comment_Text\", regexp_replace(col(\"Comment_Text\"), \", \", \" \")) \\\n",
    "                                 .withColumn(\"Comment_Text_Words\", split(col(\"Comment_Text\"), \" \"))\n",
    "\n",
    "print(\"article_csv_words: \")\n",
    "article_csv_words.show(5)\n",
    "print(\"comments_csv_words: \")\n",
    "comments_csv_words.show(5)\n",
    "\n",
    "# Clean individual Words\n",
    "print(\"article_csv_words: \")\n",
    "article_csv_words.select(\"Combined_Words\").show(5)\n",
    "print(\"comments_csv_words: \")\n",
    "comments_csv_words.show(5)\n",
    "\n",
    "print(\"Article Words  Count:\", article_csv_words.count())\n",
    "print(\"Article Words  :\", article_csv_words.show(50, truncate=True))\n",
    "print(\"Comment Words  Count:\", comments_csv_words.count())\n",
    "print(\"Comment Words  :\", comments_csv_words.show(50, truncate=True))\n",
    "print(type(article_csv_words))\n",
    "\n",
    "combined_words_df = article_csv_words.select(explode(\"Combined_Words\").alias(\"Word\"))\n",
    "# Show the first few rows to verify\n",
    "print(\"Combined Words:\")\n",
    "combined_words_df.show(50, truncate=True)\n",
    "print(\"Combined Words Count:\", combined_words_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ecc0c2-ccda-4a52-9790-feb96db405c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert words to lowercase and remove non-alphabetic characters\n",
    "def clean_word(word):\n",
    "    return ''.join(char for char in word.lower() if char.isalpha())\n",
    "\n",
    "# Register the UDF\n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "clean_word_udf = spark_session.udf.register(\"clean_word\", clean_word, StringType())\n",
    "\n",
    "# Split the 'Word' column into an array\n",
    "split_words_df = combined_words_df.withColumn(\"Split_Words\", split(combined_words_df[\"Word\"], \"[,;]\"))\n",
    "\n",
    "# Explode the array into separate rows\n",
    "exploded_words_df = split_words_df.select(explode(split_words_df[\"Split_Words\"]).alias(\"Word\"))\n",
    "\n",
    "# Apply the UDF to the 'Word' column\n",
    "cleaned_words_df = exploded_words_df.withColumn(\"Cleaned_Word\", clean_word_udf(\"Word\"))\n",
    "\n",
    "# Filter out rows where 'Cleaned_Word' is empty\n",
    "filtered_cleaned_words_df = cleaned_words_df.filter(cleaned_words_df.Cleaned_Word != '')\n",
    "\n",
    "#REDIS\n",
    "redis_client = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "\n",
    "# Count the frequency of each word before removing duplicates\n",
    "word_frequencies_df = filtered_cleaned_words_df.groupBy(\"Cleaned_Word\").count().withColumnRenamed(\"count\", \"Frequency\")\n",
    "\n",
    "save_word_frequencies_to_redis(redis_client, word_frequencies_df)\n",
    "\n",
    "# Remove duplicates based on 'Cleaned_Word'\n",
    "distinct_cleaned_words_df = filtered_cleaned_words_df.select(\"Cleaned_Word\").distinct()\n",
    "\n",
    "# Show the first few rows to verify\n",
    "print(\"Distinct Cleaned Combined Words:\")\n",
    "distinct_cleaned_words_df.show(50, truncate=True)\n",
    "print(\"Distinct Cleaned Combined Words Count:\", distinct_cleaned_words_df.count())\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "distinct_cleaned_words_df.write.option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"quoteAll\", True) \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"assignData/clean_words_data_csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
