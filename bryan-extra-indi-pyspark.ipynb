{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Lai ZhonPoa\n",
    "\"\"\"\n",
    "# Bryans individual is not limited to BryanIndividual.ipynb\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from UtilsNeo4J import DataBaseHandler\n",
    "from UtilsRedis import Redis_Utilities\n",
    "\n",
    "from GlobalSparkSession import GlobalSparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "spark = GlobalSparkSession.get_instance()\n",
    "\n",
    "# Setup Neo4j driver and Redis client\n",
    "neo4j_uri = \"neo4j+s://f2d488e8.databases.neo4j.io\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"EEftBBSnXlP8rzseU038drph7Ue5SzVVxDvlX2kL2y8\" # Replace with your actual password\n",
    "redis_utils = Redis_Utilities()\n",
    "\n",
    "db_handler = DataBaseHandler(neo4j_uri, neo4j_user, neo4j_password, redis_utils)\n",
    "\n",
    "# Get the total number of unique entries in the lexicon\n",
    "total_unique_entries = db_handler.get_total_unique_entries()\n",
    "print(f\"Total number of unique entries: {total_unique_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Visualization Functions\n",
    "def fetch_synonyms(tx, limit=25):\n",
    "    query = f\"\"\"\n",
    "    MATCH (w:Word)-[:SYNONYM]-(s:Word)\n",
    "    RETURN w.word AS word, collect(DISTINCT s.word) AS synonyms\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    result = tx.run(query)\n",
    "    return result.values()\n",
    "\n",
    "def create_synonym_network_pyspark(spark, db_handler, limit=20):\n",
    "    with db_handler.neo4j_driver.session() as session:\n",
    "        synonyms = session.execute_read(fetch_synonyms, limit)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    synonyms_df = spark.createDataFrame(synonyms, [\"word\", \"synonyms\"])\n",
    "    \n",
    "    # Explode the synonyms list into individual rows\n",
    "    exploded_df = synonyms_df.withColumn(\"synonym\", explode(col(\"synonyms\"))).select(\"word\", \"synonym\")\n",
    "    \n",
    "    # Use toLocalIterator to avoid memory issues\n",
    "    edges = [(row.word, row.synonym) for row in exploded_df.toLocalIterator()]\n",
    "    \n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_network(G):\n",
    "    pos = nx.spring_layout(G, k=0.55)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=5000, font_size=13)\n",
    "    plt.title(\"Synonym Network\")\n",
    "    plt.show()\n",
    "\n",
    "def identify_clusters(G):\n",
    "    clusters = nx.community.greedy_modularity_communities(G)\n",
    "    themes = {i: list(cluster) for i, cluster in enumerate(clusters)}\n",
    "    return themes\n",
    "\n",
    "G = create_synonym_network_pyspark(spark, db_handler, limit=30) \n",
    "visualize_network(G)\n",
    "\n",
    "themes = identify_clusters(G)\n",
    "for theme_id, words in themes.items():\n",
    "    print(f\"Theme {theme_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilsRedis import Redis_Utilities\n",
    "import redis\n",
    "def get_word_data(word_to_search):\n",
    "    redis_utils = Redis_Utilities()\n",
    "    \n",
    "    sentiment_data = redis_utils.get_sentiment(word_to_search)\n",
    "    \n",
    "    synonyms = db_handler.get_synonyms(word_to_search)\n",
    "    antonyms = db_handler.get_antonyms(word_to_search)\n",
    "    \n",
    "    print(f\"Synonyms for '{word_to_search}': {', '.join(synonyms)}\")\n",
    "    print(f\"Antonyms for '{word_to_search}': {', '.join(antonyms)}\")\n",
    "    print(f\"Sentiment for '{word_to_search}':\", sentiment_data)\n",
    "\n",
    "get_word_data(\"sedih\")\n",
    "get_word_data(\"gembira\")\n",
    "get_word_data(\"ibu\")\n",
    "get_word_data(\"hasil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "from UtilsRedis import Redis_Utilities\n",
    "\n",
    "def analyze_word_frequencies(spark, num_rows=10):\n",
    "    \"\"\"\n",
    "    Analyze and display word frequencies.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int): Number of rows to show in the preview and frequency lists.\n",
    "    \"\"\"\n",
    "    # Initialize Redis client and retrieve all frequencies\n",
    "    redis_client = Redis_Utilities()\n",
    "    frequencies = redis_utils.get_all_word_frequencies()\n",
    "\n",
    "    # Convert the dictionary to a PySpark DataFrame\n",
    "    word_frequencies_list = [{\"Cleaned_Word\": word, \"Frequency\": int(freq)} for word, freq in frequencies.items()]\n",
    "    word_frequencies_df = spark.createDataFrame(Row(**x) for x in word_frequencies_list)\n",
    "\n",
    "    # Show the preview of word frequencies\n",
    "    print(f\"Preview of all word frequencies (first {num_rows} rows):\")\n",
    "    word_frequencies_df.show(num_rows)\n",
    "\n",
    "    # Most common words\n",
    "    most_common_words_df = word_frequencies_df.orderBy(col(\"Frequency\").desc()).limit(num_rows)\n",
    "    print(\"Most common words:\")\n",
    "    most_common_words_df.show()\n",
    "\n",
    "    # Least common words\n",
    "    least_common_words_df = word_frequencies_df.orderBy(col(\"Frequency\").asc()).limit(num_rows)\n",
    "    print(\"Least common words:\")\n",
    "    least_common_words_df.show()\n",
    "\n",
    "    # Words used exactly once\n",
    "    once_used_df = word_frequencies_df.filter(col(\"Frequency\") == 1)\n",
    "    once_used = [row[\"Cleaned_Word\"] for row in once_used_df.collect()]\n",
    "    print(\"\\nWords used once:\")\n",
    "    print(\", \".join(once_used))\n",
    "\n",
    "# Call the function with the desired number of rows to display\n",
    "analyze_word_frequencies(spark, num_rows=10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
