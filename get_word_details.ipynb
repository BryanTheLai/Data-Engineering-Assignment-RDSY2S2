{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcba52-a99f-47e0-bd28-61836fa919fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580414b4-34cd-47a0-b3c0-4b87b1881664",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AIAPI=AIzaSyAni0l_OyroARliz4w49uOW_d52xsm3Hqw\n",
    "GEMINIAPI = %env AIAPI\n",
    "print(GEMINIAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac55215-fa13-471a-87a6-7ff1bf56e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType, BooleanType\n",
    "from pyspark.sql.functions import udf, explode, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Optional\n",
    "from UtilsGoogle import get_word_details\n",
    "\n",
    "# PySpark setup\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ScrapedDataProcessor\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "get_word_details([\"whavig2yv2r\"], str(GEMINIAPI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f6b34-56c1-4fe7-9810-5a79ef701b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data\n",
    "clean_words_df = spark.read.csv(\"assignData/clean_words_data_csv\", header=True)\n",
    "print(\"clean_words_df.count():\", clean_words_df.count())\n",
    "\n",
    "# Add a unique row number to the DataFrame\n",
    "window = Window.orderBy(\"Cleaned_Word\")  # Adjust orderBy as needed\n",
    "clean_words_df = clean_words_df.withColumn(\"row_number\", row_number().over(window))\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 50\n",
    "total_rows = clean_words_df.count()\n",
    "all_csv_data = []\n",
    "\n",
    "for i in range(0, total_rows, batch_size):\n",
    "    # Select a specific batch of rows\n",
    "    batch_df = clean_words_df.filter((col(\"row_number\") > i) & (col(\"row_number\") <= i + batch_size))\n",
    "    batch_words = batch_df.select(\"Cleaned_Word\").rdd.map(lambda row: row[0]).collect()\n",
    "    \n",
    "    # Call the get_word_details function with the batch\n",
    "    batch_csv_data = get_word_details(batch_words, str(GEMINIAPI))\n",
    "    \n",
    "    # Process the response\n",
    "    batch_rows = batch_csv_data.strip().split(\"\\n\")\n",
    "    if batch_rows[0].startswith('\"word\"'):\n",
    "        batch_rows = batch_rows[1:]\n",
    "    \n",
    "    all_csv_data.extend(batch_rows)\n",
    "\n",
    "# Parse rows into structured data\n",
    "parsed_data = [row.split(',') for row in all_csv_data if len(row.split(',')) == 6]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"word\", StringType(), True),\n",
    "    StructField(\"definition\", StringType(), True),\n",
    "    StructField(\"antonym\", StringType(), True),\n",
    "    StructField(\"synonym\", StringType(), True),\n",
    "    StructField(\"tatabahasa\", StringType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "all_csv_data_df = spark.createDataFrame(parsed_data, schema=schema)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"assignData/word_details_csv\"\n",
    "all_csv_data_df.write.option(\"header\", True) \\\n",
    "                     .mode(\"overwrite\") \\\n",
    "                     .csv(output_path)\n",
    "\n",
    "print(f\"Data written to {output_path}\")\n",
    "print(f\"Number of usable word scsv_data_df : {csv_data_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db529c7e-eb86-4d6b-829f-f54d05f32eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_details_csv = spark.read.csv(\"assignData/word_details_csv\", header=True)\n",
    "# Define UDF to filter out unusable words\n",
    "def is_usable(definition):\n",
    "    return \"tidak diketahui\" not in definition.lower() or \"nama\" not in definition.lower()\n",
    "\n",
    "is_usable_udf = udf(is_usable, BooleanType())\n",
    "\n",
    "# Filter usable words\n",
    "cleaned_data = word_details_csv.filter(is_usable_udf(col(\"definition\")))\n",
    "\n",
    "# Save the cleaned data to a new CSV\n",
    "cleaned_data.write.csv(\"assignData/word_details_csv_cleaned\", header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Number of usable words: {cleaned_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca697fd-6161-48d7-95e2-572f9decdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_details_csv_cleaned = spark.read.csv(\"assignData/word_details_csv_cleaned\", header=True)\n",
    "print(f\"Output of word_details_csv_cleaned.show(20): {word_details_csv_cleaned.show(20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88f836-6180-4f79-bc0e-e9a02c389e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
