{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdc80b-3017-4dd0-b652-8ff8db8ce8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "\n",
    "class WikipediaDataProcessor:\n",
    "    def __init__(self, app_name: str):\n",
    "        self.spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "    def fetch_wikipedia_articles(self, aid_values: range) -> List[Tuple[int, str, str]]:\n",
    "        url = \"https://ms.wikipedia.org/w/api.php\"  # Bahasa Melayu Wikipedia API endpoint\n",
    "        articles = []\n",
    "        print(\"Fetching Wikipedia articles...\")\n",
    "        for aid in aid_values:\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"pageids\": aid,\n",
    "                \"prop\": \"extracts\",\n",
    "                \"explaintext\": True\n",
    "            }\n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "                for page_id, page in pages.items():\n",
    "                    title = page.get(\"title\", \"\").strip()\n",
    "                    content = page.get(\"extract\", \"\").strip()\n",
    "                    if title and content:\n",
    "                        articles.append((int(page_id), title, content))\n",
    "                        print(f\"Fetched: {title}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch article for AID {aid}: {e}\")\n",
    "        print(\"Fetching completed.\")\n",
    "        return articles\n",
    "\n",
    "    def process_articles(self, aid_values: range, output_path: str):\n",
    "        # Fetch articles\n",
    "        articles_data = self.fetch_wikipedia_articles(aid_values)\n",
    "\n",
    "        # Define schema\n",
    "        article_schema = StructType([\n",
    "            StructField(\"ArticleID\", IntegerType(), True),\n",
    "            StructField(\"Title\", StringType(), True),\n",
    "            StructField(\"Content\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "        # Create DataFrame\n",
    "        articles_df = self.spark.createDataFrame(articles_data, schema=article_schema)\n",
    "\n",
    "        # Filter out rows with empty Title or Content\n",
    "        filtered_df = articles_df.filter(\n",
    "            (articles_df[\"Title\"] != \"\") & (articles_df[\"Content\"] != \"\")\n",
    "        )\n",
    "\n",
    "        # Select top 50 articles\n",
    "        articles_df_limited = filtered_df.select(\"Title\", \"Content\").limit(50)\n",
    "\n",
    "        # Show the first 50 records\n",
    "        print(\"Displaying the first 50 records:\")\n",
    "        articles_df_limited.show(truncate=False)\n",
    "\n",
    "        # Save to CSV\n",
    "        articles_df_limited.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(output_path)\n",
    "        print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    processor = WikipediaDataProcessor(\"WikipediaDataProcessor\")\n",
    "    aid_values = range(2000, 2100)  # Adjust range as needed\n",
    "    output_path = \"assignmentData/wikipedia_articles_top50.csv\"\n",
    "    processor.process_articles(aid_values, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fdc5b-c22c-43fa-afb3-257c7d5db09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, regexp_replace\n",
    "\n",
    "class WikipediaCSVProcessor:\n",
    "    def __init__(self, app_name: str):\n",
    "        self.spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "    def tokenize_csv_content(self, input_path: str, output_path: str):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        articles_df = self.spark.read.format(\"csv\").option(\"header\", \"true\").load(input_path)\n",
    "\n",
    "        # Ensure necessary columns are present\n",
    "        if \"Content\" not in articles_df.columns or \"Title\" not in articles_df.columns:\n",
    "            raise ValueError(\"Input CSV must contain 'Title' and 'Content' columns.\")\n",
    "\n",
    "        # Remove unwanted symbols and tokenize into individual words\n",
    "        tokenized_df = articles_df.withColumn(\n",
    "            \"Cleaned_Word\",\n",
    "            explode(\n",
    "                split(\n",
    "                    regexp_replace(col(\"Content\"), r\"[,\\\"\\'\\\\-]\", \"\"),  # Remove specific symbols: , \" ' -\n",
    "                    \"\\\\s+\"  # Split by whitespace\n",
    "                )\n",
    "            )\n",
    "        ).select(\"Title\", \"Cleaned_Word\")\n",
    "\n",
    "        # Show tokenized content\n",
    "        print(\"Displaying tokenized content:\")\n",
    "        tokenized_df.show(truncate=False)\n",
    "\n",
    "        # Save the tokenized content to a new CSV file\n",
    "        tokenized_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(output_path)\n",
    "        print(f\"Tokenized data saved to {output_path}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    processor = WikipediaCSVProcessor(\"WikipediaCSVProcessor\")\n",
    "    input_path = \"assignmentData/wikipedia_articles_top50.csv\"  # Path to the input CSV file\n",
    "    output_path = \"assignmentData/wikipedia_articles_tokenized.csv\"  # Path to save the tokenized CSV file\n",
    "    processor.tokenize_csv_content(input_path, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49de136-45dd-4d67-95cb-86a725a7babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, regexp_replace\n",
    "import os\n",
    "\n",
    "class WikipediaStreamingProcessor:\n",
    "    def __init__(self, app_name: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def process_streaming_data(self, input_dir: str, output_dir: str):\n",
    "        # Read the streaming data from the input directory\n",
    "        streaming_df = self.spark.readStream \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(input_dir)\n",
    "\n",
    "        # Ensure necessary columns are present\n",
    "        if \"Title\" not in streaming_df.columns or \"Cleaned_Word\" not in streaming_df.columns:\n",
    "            raise ValueError(\"Input CSV must contain 'Title' and 'Cleaned_Word' columns.\")\n",
    "\n",
    "        # Perform further processing if needed (e.g., filtering, transformations)\n",
    "        processed_df = streaming_df.select(\"Title\", \"Cleaned_Word\")\n",
    "\n",
    "        # Write the streaming results to the output directory\n",
    "        query = processed_df.writeStream \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"path\", output_dir) \\\n",
    "            .option(\"checkpointLocation\", f\"{output_dir}/checkpoint\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start()\n",
    "\n",
    "        print(f\"Streaming job started. Writing data to: {output_dir}\")\n",
    "\n",
    "        # Block and wait for the streaming to finish\n",
    "        query.awaitTermination()\n",
    "\n",
    "        # Verify if files are written to the output directory\n",
    "        if os.path.exists(output_dir) and os.listdir(output_dir):\n",
    "            print(f\"Files saved successfully in {output_dir}.\")\n",
    "        else:\n",
    "            print(f\"No files were saved in {output_dir}.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    processor = WikipediaStreamingProcessor(\"WikipediaStreamingProcessor\")\n",
    "    input_dir = \"assignmentData/wikipedia_articles_tokenized.csv\"  # Directory for tokenized input\n",
    "    output_dir = \"assignmentData/Data_Streaming\"  # Directory to save streaming output\n",
    "    processor.process_streaming_data(input_dir, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
